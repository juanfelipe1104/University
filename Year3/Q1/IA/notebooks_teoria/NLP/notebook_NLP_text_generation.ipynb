{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b9be69a8-4f30-4556-a659-299d4f53e1e8",
      "metadata": {
        "id": "b9be69a8-4f30-4556-a659-299d4f53e1e8"
      },
      "source": [
        "# Text generation with an RNN\n",
        "This notebook is heavily based on the excellent blog [\"The Unreasonable Effectiveness of Recurrent Neural Networks\"](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) by Andrej Karpathy (which you must read) and the Keras example [\"Character-level text generation with LSTM\"](https://keras.io/examples/generative/lstm_character_level_text_generation/) by François Chollet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "96700095-0561-457c-87b5-6e60a5ba0fa8",
      "metadata": {
        "id": "96700095-0561-457c-87b5-6e60a5ba0fa8"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "import numpy as np\n",
        "import random\n",
        "import io"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06fa580a-ed9f-4906-941c-72cac0946cee",
      "metadata": {
        "id": "06fa580a-ed9f-4906-941c-72cac0946cee"
      },
      "source": [
        "## Load a text from [Project Gutenberg](https://www.gutenberg.org/)\n",
        "Project Gutenberg is a library of over 70,000 free eBooks\n",
        "\n",
        "Here we shall load \"*Alice's Adventures in Wonderland*\" by Lewis Carroll"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f0cc11e9-46fc-497e-9eb1-427c3d1c6e4c",
      "metadata": {
        "id": "f0cc11e9-46fc-497e-9eb1-427c3d1c6e4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f13d3c1-0e01-474d-c65a-6409a3c02a3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.gutenberg.org/cache/epub/11/pg11.txt\n",
            "\u001b[1m174355/174355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2us/step\n",
            "Corpus length: 163916\n"
          ]
        }
      ],
      "source": [
        "url = 'https://www.gutenberg.org/cache/epub/11/pg11.txt'\n",
        "path = keras.utils.get_file(origin=url)\n",
        "\n",
        "with io.open(path, encoding=\"utf-8\") as f:\n",
        "    text = f.read().lower()\n",
        "text = text.replace(\"\\n\", \" \")  # We remove newline chars for nicer display\n",
        "print(\"Corpus length:\", len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "919d9784-f763-4f65-8647-d8b2ca687a3b",
      "metadata": {
        "id": "919d9784-f763-4f65-8647-d8b2ca687a3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb5ba9dd-0f1b-4e1c-8857-36a6eed7143e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number unique characters: 64\n",
            "Number of sequences: 54618\n"
          ]
        }
      ],
      "source": [
        "chars = sorted(list(set(text)))\n",
        "print(\"Number unique characters:\", len(chars))\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "# cut the text in semi-redundant sequences of maxlen characters\n",
        "maxlen = 64\n",
        "step = 3\n",
        "\n",
        "sentences = []\n",
        "next_chars = []\n",
        "for i in range(0, len(text) - maxlen, step):\n",
        "    sentences.append(text[i : i + maxlen])\n",
        "    next_chars.append(text[i + maxlen])\n",
        "print(\"Number of sequences:\", len(sentences))\n",
        "\n",
        "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=\"bool\")\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=\"bool\")\n",
        "\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        x[i, t, char_indices[char]] = 1\n",
        "    y[i, char_indices[next_chars[i]]] = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45f45d56-541b-4fc7-aa9d-02ae13d349e4",
      "metadata": {
        "id": "45f45d56-541b-4fc7-aa9d-02ae13d349e4"
      },
      "source": [
        "# A very simple LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f452f5cf-9fee-49fd-8ced-5888aee1b77b",
      "metadata": {
        "id": "f452f5cf-9fee-49fd-8ced-5888aee1b77b"
      },
      "outputs": [],
      "source": [
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(maxlen, len(chars))),\n",
        "        layers.LSTM(128),\n",
        "        layers.Dense(len(chars), activation=\"softmax\"),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f9bd7656-eb7c-4c91-a6c7-ff6bfc1b8792",
      "metadata": {
        "id": "f9bd7656-eb7c-4c91-a6c7-ff6bfc1b8792"
      },
      "outputs": [],
      "source": [
        "optimizer = keras.optimizers.RMSprop(learning_rate=0.01)\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\",\n",
        "              optimizer=optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc9eda16-1220-43db-bce5-122cbdeba1db",
      "metadata": {
        "id": "bc9eda16-1220-43db-bce5-122cbdeba1db"
      },
      "source": [
        "# Fit (train) the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "cf63cf78-f09b-4039-8b08-b39006226be5",
      "metadata": {
        "id": "cf63cf78-f09b-4039-8b08-b39006226be5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5f8604d-23d1-43c2-a891-dece2946e499"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m427/427\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 156ms/step - loss: 2.6886\n",
            "Epoch 2/10\n",
            "\u001b[1m427/427\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 155ms/step - loss: 1.8968\n",
            "Epoch 3/10\n",
            "\u001b[1m427/427\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 152ms/step - loss: 1.6436\n",
            "Epoch 4/10\n",
            "\u001b[1m427/427\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 150ms/step - loss: 1.5011\n",
            "Epoch 5/10\n",
            "\u001b[1m427/427\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 151ms/step - loss: 1.4087\n",
            "Epoch 6/10\n",
            "\u001b[1m427/427\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 151ms/step - loss: 1.3074\n",
            "Epoch 7/10\n",
            "\u001b[1m427/427\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 152ms/step - loss: 1.2498\n",
            "Epoch 8/10\n",
            "\u001b[1m427/427\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 153ms/step - loss: 1.1906\n",
            "Epoch 9/10\n",
            "\u001b[1m427/427\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 151ms/step - loss: 1.1481\n",
            "Epoch 10/10\n",
            "\u001b[1m427/427\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 152ms/step - loss: 1.1042\n",
            "CPU times: user 16min 50s, sys: 38.5 s, total: 17min 29s\n",
            "Wall time: 11min 10s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7cf050379640>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "epochs = 10\n",
        "batch_size = 128\n",
        "\n",
        "model.fit(x, y,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b73ad3c5-b0b5-4281-86f7-2b2689aaaecf",
      "metadata": {
        "id": "b73ad3c5-b0b5-4281-86f7-2b2689aaaecf"
      },
      "source": [
        "# Predict for the temperatures `[0.01, 0.5, 0.7, 1]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "9353cdb6-83db-4212-8a2a-39d636132e41",
      "metadata": {
        "id": "9353cdb6-83db-4212-8a2a-39d636132e41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f52b97ef-d9e9-41dd-f563-0e5f2c7e7a0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Temperature: 0.01\n",
            "of the sabbit all the same was been to say the court was a great of the sabbit all the same was been to say the court was a great of the sabbit all the same was been to say the court was a great of the sabbit all the same was been to say the court was a great of the sabbit all the same was been to say the court was a great of the sabbit all the same was been to say the court was a great of the sabbit all the same was been to say the court was a great of the sabbit all the same was been to say the court was \n",
            "\n",
            "Temperature: 0.5\n",
            "of liciuse, to the two stain, and began sortan as the was engloding in at of and reseration of course,” the modaration ordered of the used out of them, and sometige to make on of sausent, and then the court!” and she to knower and how down to say the other of setter and come, and the trademanimakion, how promise of the sadden up a donations and wast the end, and the poor with a minutes and this was to the used and just be a convertations of the said, but no head the chisk to get on the work oreant, and had \n",
            "\n",
            "Temperature: 0.7\n",
            "one as she be a hould back and the project gutenberg™ freesed as she was was in a very in that to be on into the rabbit full, and the millace, withouthis ofet to the comfular electronic and them nearles of them with my hatter in at the atwear; as she did not came very tone at lewing it size ohe had presting made a little gimpy proved get of the flather again!” said alice; “i had be and hard confusing and mink,” said alice, “that was being more an impite of and more same from and growing on a little first an\n",
            "\n",
            "Temperature: 1\n",
            "book, libg wast at it was, agait.—   as she rations and wait it, and there would cas; and heirss and got oncem)nat despuarramh. “you abe!” alice gill know. “and my, like off and making thems long re copy upon her voice.  she had never hes.  put in a roold dereating did days same wathing would, and get very bach with quetar. of say talking wook at it work, and to, and seem.  “dedy in you just is suble to prode_s 5hand much grown you must be had back in al cleasing on essift or t. she fornow at her, all owe a\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def sample(softmax, temperature=1.0): # default temperature is 1\n",
        "    softmax = np.asarray(softmax).astype(\"float64\")\n",
        "    logits = np.log(softmax) / temperature # apply temperature scaling\n",
        "    exp_preds = np.exp(logits)\n",
        "    softmax = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, softmax, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "number_of_letters_to_generate = 512\n",
        "# pick a random starting point from somewhere in the text\n",
        "start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "\n",
        "for temperature in [0.01, 0.5, 0.7, 1]:  #  the hotter the more nonsense\n",
        "    print(\"Temperature:\", temperature)\n",
        "\n",
        "    #sentence = text[start_index : start_index + maxlen]\n",
        "    # this particular seed sentence is specific to \"Alice's Adventures in Wonderland\"\n",
        "    sentence = \"after time she heard a little pattering of feet in the distance \" # len = 64, i.e. maxlen\n",
        "\n",
        "    generated = \"\"\n",
        "    for i in range(number_of_letters_to_generate):\n",
        "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "        for t, char in enumerate(sentence):\n",
        "            x_pred[0, t, char_indices[char]] = 1.0\n",
        "        preds = model.predict(x_pred, verbose=0)[0] # returns softmax\n",
        "        next_index = sample(preds, temperature)\n",
        "        next_char = indices_char[next_index]\n",
        "        sentence = sentence[1:] + next_char\n",
        "        generated += next_char\n",
        "\n",
        "    print(generated)\n",
        "    print(\"\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}